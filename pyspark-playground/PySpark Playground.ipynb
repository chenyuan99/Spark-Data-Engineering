{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# PySpark Playground\n",
    "\n",
    "A hands-on reference notebook covering the most common PySpark patterns.\n",
    "\n",
    "| Section | Topics |\n",
    "|---------|--------|\n",
    "| 0 | Setup & SparkSession |\n",
    "| 1 | DataFrame Basics |\n",
    "| 2 | Selecting & Filtering |\n",
    "| 3 | Aggregations & GroupBy |\n",
    "| 4 | Joins |\n",
    "| 5 | Window Functions |\n",
    "| 6 | Spark SQL |\n",
    "| 7 | User-Defined Functions (UDFs) |\n",
    "| 8 | Null Handling & Data Quality |\n",
    "| 9 | String & Date Operations |\n",
    "| 10 | Reading & Writing Data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section0_header",
   "metadata": {},
   "source": [
    "## 0. Setup & SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark==3.5.8 python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ab33000817345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"]   = r\"C:\\devhome\\tools\\Java\\jdk-17.0.2\"\n",
    "os.environ[\"SPARK_HOME\"]  = r\"C:\\devhome\\tools\\spark-3.5.8-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\devhome\\tools\\hadoop-3.3.6\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"PySpark Playground\")\n",
    "    .config(\"spark.pyspark.python\", os.environ.get(\"PYSPARK_PYTHON\", \"\"))\n",
    "    .config(\"spark.pyspark.driver.python\", os.environ.get(\"PYSPARK_DRIVER_PYTHON\", \"\"))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")  # keeps local runs fast\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. DataFrame Basics\n",
    "\n",
    "Creating DataFrames from Python collections and inspecting schema/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section1_create",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# --- Option A: infer schema from list of tuples ---\n",
    "employees_data = [\n",
    "    (1, \"Alice\",   \"Engineering\", 95000.0, date(2019, 3, 15)),\n",
    "    (2, \"Bob\",     \"Marketing\",   72000.0, date(2020, 7, 1)),\n",
    "    (3, \"Carol\",   \"Engineering\", 110000.0, date(2017, 11, 20)),\n",
    "    (4, \"Dave\",    \"Marketing\",   68000.0, date(2021, 1, 10)),\n",
    "    (5, \"Eve\",     \"Engineering\", 105000.0, date(2018, 5, 5)),\n",
    "    (6, \"Frank\",   \"HR\",          60000.0, date(2022, 2, 28)),\n",
    "    (7, \"Grace\",   \"HR\",          63000.0, date(2020, 9, 14)),\n",
    "    (8, \"Hank\",    \"Engineering\", 98000.0, date(2019, 6, 1)),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",         IntegerType(), False),\n",
    "    StructField(\"name\",       StringType(),  False),\n",
    "    StructField(\"department\", StringType(),  True),\n",
    "    StructField(\"salary\",     DoubleType(),  True),\n",
    "    StructField(\"hire_date\",  DateType(),    True),\n",
    "])\n",
    "\n",
    "employees = spark.createDataFrame(employees_data, schema)\n",
    "\n",
    "# --- Option B: from list of Row objects ---\n",
    "departments = spark.createDataFrame([\n",
    "    Row(dept_id=\"Engineering\", budget=500000, location=\"San Francisco\"),\n",
    "    Row(dept_id=\"Marketing\",   budget=200000, location=\"New York\"),\n",
    "    Row(dept_id=\"HR\",          budget=150000, location=\"Chicago\"),\n",
    "])\n",
    "\n",
    "print(\"=== employees schema ===\")\n",
    "employees.printSchema()\n",
    "\n",
    "print(\"=== employees sample ===\")\n",
    "employees.show()\n",
    "\n",
    "print(f\"Row count: {employees.count()}\")\n",
    "print(f\"Columns:   {employees.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section1_describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "employees.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Selecting & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2_select",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, expr, when\n",
    "\n",
    "# Select specific columns (three equivalent ways)\n",
    "employees.select(\"name\", \"department\", \"salary\").show()\n",
    "\n",
    "# Add a derived column\n",
    "employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    (col(\"salary\") * 1.1).alias(\"salary_with_raise\"),\n",
    "    when(col(\"salary\") >= 90000, \"senior\").otherwise(\"junior\").alias(\"level\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section2_filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter / where (interchangeable)\n",
    "print(\"Engineering employees earning > 95k:\")\n",
    "employees.filter(\n",
    "    (col(\"department\") == \"Engineering\") & (col(\"salary\") > 95000)\n",
    ").show()\n",
    "\n",
    "# isin\n",
    "print(\"Marketing or HR employees:\")\n",
    "employees.where(col(\"department\").isin(\"Marketing\", \"HR\")).show()\n",
    "\n",
    "# String pattern match\n",
    "print(\"Names containing 'a' (case-insensitive):\")\n",
    "employees.filter(col(\"name\").rlike(\"(?i)a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Aggregations & GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section3_agg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, min, max, stddev, round as spark_round\n",
    "\n",
    "dept_stats = (\n",
    "    employees\n",
    "    .groupBy(\"department\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"headcount\"),\n",
    "        spark_round(avg(\"salary\"), 0).alias(\"avg_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\"),\n",
    "        spark_round(stddev(\"salary\"), 0).alias(\"stddev_salary\"),\n",
    "    )\n",
    "    .orderBy(\"department\")\n",
    ")\n",
    "\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section3_pivot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: department salaries as columns (requires small cardinality)\n",
    "from pyspark.sql.functions import avg as avg_fn\n",
    "\n",
    "employees.groupBy().pivot(\"department\").agg(spark_round(avg_fn(\"salary\"), 0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section4_joins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join â€“ enrich employees with department metadata\n",
    "enriched = employees.join(departments, employees.department == departments.dept_id, how=\"inner\")\n",
    "enriched.select(\"name\", \"department\", \"salary\", \"budget\", \"location\").show()\n",
    "\n",
    "# Left join (employees without a matching department row still appear)\n",
    "employees_left = employees.join(departments, employees.department == departments.dept_id, how=\"left\")\n",
    "employees_left.select(\"name\", \"department\", \"location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section4_self_join",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-join: pair employees in the same department\n",
    "emp_a = employees.alias(\"a\")\n",
    "emp_b = employees.alias(\"b\")\n",
    "\n",
    "(\n",
    "    emp_a.join(emp_b,\n",
    "               (col(\"a.department\") == col(\"b.department\")) & (col(\"a.id\") < col(\"b.id\")),\n",
    "               how=\"inner\")\n",
    "    .select(\n",
    "        col(\"a.name\").alias(\"employee_1\"),\n",
    "        col(\"b.name\").alias(\"employee_2\"),\n",
    "        col(\"a.department\")\n",
    "    )\n",
    "    .orderBy(\"department\", \"employee_1\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Window Functions\n",
    "\n",
    "Window functions compute a value for each row based on a group of related rows (the *window*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section5_rank",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number, lag, lead, sum as sum_fn, avg as avg_fn2\n",
    "\n",
    "# Partition by department, order by salary descending\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    rank().over(dept_window).alias(\"rank\"),\n",
    "    dense_rank().over(dept_window).alias(\"dense_rank\"),\n",
    "    row_number().over(dept_window).alias(\"row_num\"),\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section5_lag_lead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag / lead: compare salary to next/previous colleague in same dept\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    lag(\"salary\", 1).over(dept_window).alias(\"prev_salary\"),\n",
    "    lead(\"salary\", 1).over(dept_window).alias(\"next_salary\"),\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section5_running_total",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running total of salary within department (order matters)\n",
    "running_window = Window.partitionBy(\"department\").orderBy(\"hire_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"hire_date\", \"salary\",\n",
    "    sum_fn(\"salary\").over(running_window).alias(\"cumulative_salary\")\n",
    ").orderBy(\"department\", \"hire_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Spark SQL\n",
    "\n",
    "Register DataFrames as temp views and query with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section6_sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees.createOrReplaceTempView(\"employees\")\n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Top earner per department\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, name, salary\n",
    "    FROM (\n",
    "        SELECT department, name, salary,\n",
    "               RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rnk\n",
    "        FROM employees\n",
    "    )\n",
    "    WHERE rnk = 1\n",
    "    ORDER BY department\n",
    "\"\"\").show()\n",
    "\n",
    "# Department budget vs total payroll\n",
    "spark.sql(\"\"\"\n",
    "    SELECT d.dept_id, d.budget, d.location,\n",
    "           COUNT(e.id)              AS headcount,\n",
    "           ROUND(SUM(e.salary), 0)  AS total_payroll,\n",
    "           ROUND(d.budget - SUM(e.salary), 0) AS remaining_budget\n",
    "    FROM departments d\n",
    "    LEFT JOIN employees e ON e.department = d.dept_id\n",
    "    GROUP BY d.dept_id, d.budget, d.location\n",
    "    ORDER BY d.dept_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. User-Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section7_udf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- Row-level Python UDF (slower, but flexible) ---\n",
    "def salary_band(salary: float) -> str:\n",
    "    if salary is None:\n",
    "        return \"unknown\"\n",
    "    if salary < 70000:\n",
    "        return \"band-A\"\n",
    "    if salary < 90000:\n",
    "        return \"band-B\"\n",
    "    return \"band-C\"\n",
    "\n",
    "salary_band_udf = udf(salary_band, StringType())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"salary\",\n",
    "    salary_band_udf(col(\"salary\")).alias(\"band\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section7_pandas_udf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vectorised Pandas UDF (much faster for large data) ---\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def normalise_salary(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalise salaries to 0-1 range within each batch.\"\"\"\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"salary\",\n",
    "    spark_round(normalise_salary(col(\"salary\")), 4).alias(\"norm_salary\")\n",
    ").orderBy(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Null Handling & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8_nulls",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, isnull, isnan, count as count_fn\n",
    "\n",
    "# Introduce some nulls for demonstration\n",
    "dirty_data = [\n",
    "    (1, \"Alice\",  95000.0),\n",
    "    (2, \"Bob\",    None),\n",
    "    (3, None,     72000.0),\n",
    "    (4, \"Dave\",   float(\"nan\")),\n",
    "    (5, \"Eve\",    105000.0),\n",
    "]\n",
    "dirty_df = spark.createDataFrame(dirty_data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "print(\"=== Raw data ===\")\n",
    "dirty_df.show()\n",
    "\n",
    "# Count nulls per column\n",
    "print(\"=== Null counts ===\")\n",
    "dirty_df.select([\n",
    "    count_fn(when(isnull(c) | isnan(c), c)).alias(c)\n",
    "    for c in dirty_df.columns\n",
    "]).show()\n",
    "\n",
    "# Fill / drop\n",
    "print(\"=== After fillna ===\")\n",
    "dirty_df.fillna({\"name\": \"Unknown\", \"salary\": 0.0}).show()\n",
    "\n",
    "print(\"=== After dropna ===\")\n",
    "dirty_df.dropna().show()\n",
    "\n",
    "# coalesce: first non-null wins\n",
    "print(\"=== coalesce salary with default ===\")\n",
    "dirty_df.select(\n",
    "    \"name\",\n",
    "    coalesce(col(\"salary\"), lit(50000.0)).alias(\"salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. String & Date Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section9_strings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    upper, lower, trim, length, concat, concat_ws,\n",
    "    regexp_replace, split, substring, initcap\n",
    ")\n",
    "\n",
    "employees.select(\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    length(col(\"name\")).alias(\"name_len\"),\n",
    "    concat_ws(\" @ \", col(\"name\"), col(\"department\")).alias(\"name_dept\"),\n",
    "    regexp_replace(col(\"department\"), \"Engineering\", \"Eng\").alias(\"dept_short\"),\n",
    "    substring(col(\"name\"), 1, 3).alias(\"initials\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section9_dates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    year, month, dayofweek, datediff, months_between,\n",
    "    current_date, date_add, date_format, to_date\n",
    ")\n",
    "\n",
    "employees.select(\n",
    "    \"name\",\n",
    "    \"hire_date\",\n",
    "    year(\"hire_date\").alias(\"hire_year\"),\n",
    "    month(\"hire_date\").alias(\"hire_month\"),\n",
    "    dayofweek(\"hire_date\").alias(\"day_of_week\"),          # 1=Sunday\n",
    "    datediff(current_date(), col(\"hire_date\")).alias(\"days_employed\"),\n",
    "    spark_round(months_between(current_date(), col(\"hire_date\")), 1).alias(\"months_employed\"),\n",
    "    date_format(col(\"hire_date\"), \"MMM dd, yyyy\").alias(\"formatted\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Reading & Writing Data\n",
    "\n",
    "Write to common formats and read them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, os\n",
    "\n",
    "tmp = tempfile.mkdtemp()\n",
    "\n",
    "# --- Parquet (columnar, compressed, schema-preserving) ---\n",
    "parquet_path = os.path.join(tmp, \"employees.parquet\")\n",
    "employees.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "df_from_parquet = spark.read.parquet(parquet_path)\n",
    "print(\"Read back from Parquet:\")\n",
    "df_from_parquet.printSchema()\n",
    "df_from_parquet.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSV ---\n",
    "csv_path = os.path.join(tmp, \"employees.csv\")\n",
    "employees.write.mode(\"overwrite\").option(\"header\", True).csv(csv_path)\n",
    "\n",
    "df_from_csv = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "print(\"Read back from CSV:\")\n",
    "df_from_csv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_partitioned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Partitioned write (Hive-style) ---\n",
    "partitioned_path = os.path.join(tmp, \"employees_by_dept\")\n",
    "(\n",
    "    employees\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"department\")\n",
    "    .parquet(partitioned_path)\n",
    ")\n",
    "\n",
    "# Spark automatically prunes partitions when filtering on the partition column\n",
    "df_eng = spark.read.parquet(partitioned_path).filter(col(\"department\") == \"Engineering\")\n",
    "print(\"Engineering employees (partition-pruned read):\")\n",
    "df_eng.show()\n",
    "\n",
    "print(f\"\\nTemp files written to: {tmp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teardown_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teardown_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done to release resources\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
