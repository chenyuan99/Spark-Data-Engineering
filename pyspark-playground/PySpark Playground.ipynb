{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title_cell",
   "metadata": {},
   "source": [
    "# PySpark Playground\n",
    "\n",
    "A hands-on reference notebook covering the most common PySpark patterns.\n",
    "\n",
    "| Section | Topics |\n",
    "|---------|--------|\n",
    "| 0 | Setup & SparkSession |\n",
    "| 1 | DataFrame Basics |\n",
    "| 2 | Selecting & Filtering |\n",
    "| 3 | Aggregations & GroupBy |\n",
    "| 4 | Joins |\n",
    "| 5 | Window Functions |\n",
    "| 6 | Spark SQL |\n",
    "| 7 | User-Defined Functions (UDFs) |\n",
    "| 8 | Null Handling & Data Quality |\n",
    "| 9 | String & Date Operations |\n",
    "| 10 | Reading & Writing Data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section0_header",
   "metadata": {},
   "source": [
    "## 0. Setup & SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.8 python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "975ab33000817345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.8\n",
      "Spark UI: http://localhost:4040\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"JAVA_HOME\"]   = r\"C:\\devhome\\tools\\Java\\jdk-17.0.2\"\n",
    "os.environ[\"SPARK_HOME\"]  = r\"C:\\devhome\\tools\\spark-3.5.8-bin-hadoop3\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\devhome\\tools\\hadoop-3.3.6\"\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"PySpark Playground\")\n",
    "    .config(\"spark.pyspark.python\", os.environ.get(\"PYSPARK_PYTHON\", \"\"))\n",
    "    .config(\"spark.pyspark.driver.python\", os.environ.get(\"PYSPARK_DRIVER_PYTHON\", \"\"))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\")  # keeps local runs fast\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. DataFrame Basics\n",
    "\n",
    "Creating DataFrames from Python collections and inspecting schema/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "section1_create",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== employees schema ===\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      "\n",
      "=== employees sample ===\n",
      "+---+-----+-----------+--------+----------+\n",
      "| id| name| department|  salary| hire_date|\n",
      "+---+-----+-----------+--------+----------+\n",
      "|  1|Alice|Engineering| 95000.0|2019-03-15|\n",
      "|  2|  Bob|  Marketing| 72000.0|2020-07-01|\n",
      "|  3|Carol|Engineering|110000.0|2017-11-20|\n",
      "|  4| Dave|  Marketing| 68000.0|2021-01-10|\n",
      "|  5|  Eve|Engineering|105000.0|2018-05-05|\n",
      "|  6|Frank|         HR| 60000.0|2022-02-28|\n",
      "|  7|Grace|         HR| 63000.0|2020-09-14|\n",
      "|  8| Hank|Engineering| 98000.0|2019-06-01|\n",
      "+---+-----+-----------+--------+----------+\n",
      "\n",
      "Row count: 8\n",
      "Columns:   ['id', 'name', 'department', 'salary', 'hire_date']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# --- Option A: infer schema from list of tuples ---\n",
    "employees_data = [\n",
    "    (1, \"Alice\",   \"Engineering\", 95000.0, date(2019, 3, 15)),\n",
    "    (2, \"Bob\",     \"Marketing\",   72000.0, date(2020, 7, 1)),\n",
    "    (3, \"Carol\",   \"Engineering\", 110000.0, date(2017, 11, 20)),\n",
    "    (4, \"Dave\",    \"Marketing\",   68000.0, date(2021, 1, 10)),\n",
    "    (5, \"Eve\",     \"Engineering\", 105000.0, date(2018, 5, 5)),\n",
    "    (6, \"Frank\",   \"HR\",          60000.0, date(2022, 2, 28)),\n",
    "    (7, \"Grace\",   \"HR\",          63000.0, date(2020, 9, 14)),\n",
    "    (8, \"Hank\",    \"Engineering\", 98000.0, date(2019, 6, 1)),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\",         IntegerType(), False),\n",
    "    StructField(\"name\",       StringType(),  False),\n",
    "    StructField(\"department\", StringType(),  True),\n",
    "    StructField(\"salary\",     DoubleType(),  True),\n",
    "    StructField(\"hire_date\",  DateType(),    True),\n",
    "])\n",
    "\n",
    "employees = spark.createDataFrame(employees_data, schema)\n",
    "\n",
    "# --- Option B: from list of Row objects ---\n",
    "departments = spark.createDataFrame([\n",
    "    Row(dept_id=\"Engineering\", budget=500000, location=\"San Francisco\"),\n",
    "    Row(dept_id=\"Marketing\",   budget=200000, location=\"New York\"),\n",
    "    Row(dept_id=\"HR\",          budget=150000, location=\"Chicago\"),\n",
    "])\n",
    "\n",
    "print(\"=== employees schema ===\")\n",
    "employees.printSchema()\n",
    "\n",
    "print(\"=== employees sample ===\")\n",
    "employees.show()\n",
    "\n",
    "print(f\"Row count: {employees.count()}\")\n",
    "print(f\"Columns:   {employees.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "section1_describe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           salary|\n",
      "+-------+-----------------+\n",
      "|  count|                8|\n",
      "|   mean|          83875.0|\n",
      "| stddev|20180.87850275248|\n",
      "|    min|          60000.0|\n",
      "|    max|         110000.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Descriptive statistics\n",
    "employees.describe(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Selecting & Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "section2_select",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+\n",
      "| name| department|  salary|\n",
      "+-----+-----------+--------+\n",
      "|Alice|Engineering| 95000.0|\n",
      "|  Bob|  Marketing| 72000.0|\n",
      "|Carol|Engineering|110000.0|\n",
      "| Dave|  Marketing| 68000.0|\n",
      "|  Eve|Engineering|105000.0|\n",
      "|Frank|         HR| 60000.0|\n",
      "|Grace|         HR| 63000.0|\n",
      "| Hank|Engineering| 98000.0|\n",
      "+-----+-----------+--------+\n",
      "\n",
      "+-----+--------+------------------+------+\n",
      "| name|  salary| salary_with_raise| level|\n",
      "+-----+--------+------------------+------+\n",
      "|Alice| 95000.0|104500.00000000001|senior|\n",
      "|  Bob| 72000.0|           79200.0|junior|\n",
      "|Carol|110000.0|121000.00000000001|senior|\n",
      "| Dave| 68000.0|           74800.0|junior|\n",
      "|  Eve|105000.0|115500.00000000001|senior|\n",
      "|Frank| 60000.0|           66000.0|junior|\n",
      "|Grace| 63000.0|           69300.0|junior|\n",
      "| Hank| 98000.0|107800.00000000001|senior|\n",
      "+-----+--------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, expr, when\n",
    "\n",
    "# Select specific columns (three equivalent ways)\n",
    "employees.select(\"name\", \"department\", \"salary\").show()\n",
    "\n",
    "# Add a derived column\n",
    "employees.select(\n",
    "    col(\"name\"),\n",
    "    col(\"salary\"),\n",
    "    (col(\"salary\") * 1.1).alias(\"salary_with_raise\"),\n",
    "    when(col(\"salary\") >= 90000, \"senior\").otherwise(\"junior\").alias(\"level\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "section2_filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering employees earning > 95k:\n",
      "+---+-----+-----------+--------+----------+\n",
      "| id| name| department|  salary| hire_date|\n",
      "+---+-----+-----------+--------+----------+\n",
      "|  3|Carol|Engineering|110000.0|2017-11-20|\n",
      "|  5|  Eve|Engineering|105000.0|2018-05-05|\n",
      "|  8| Hank|Engineering| 98000.0|2019-06-01|\n",
      "+---+-----+-----------+--------+----------+\n",
      "\n",
      "Marketing or HR employees:\n",
      "+---+-----+----------+-------+----------+\n",
      "| id| name|department| salary| hire_date|\n",
      "+---+-----+----------+-------+----------+\n",
      "|  2|  Bob| Marketing|72000.0|2020-07-01|\n",
      "|  4| Dave| Marketing|68000.0|2021-01-10|\n",
      "|  6|Frank|        HR|60000.0|2022-02-28|\n",
      "|  7|Grace|        HR|63000.0|2020-09-14|\n",
      "+---+-----+----------+-------+----------+\n",
      "\n",
      "Names containing 'a' (case-insensitive):\n",
      "+---+-----+-----------+--------+----------+\n",
      "| id| name| department|  salary| hire_date|\n",
      "+---+-----+-----------+--------+----------+\n",
      "|  1|Alice|Engineering| 95000.0|2019-03-15|\n",
      "|  3|Carol|Engineering|110000.0|2017-11-20|\n",
      "|  4| Dave|  Marketing| 68000.0|2021-01-10|\n",
      "|  6|Frank|         HR| 60000.0|2022-02-28|\n",
      "|  7|Grace|         HR| 63000.0|2020-09-14|\n",
      "|  8| Hank|Engineering| 98000.0|2019-06-01|\n",
      "+---+-----+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter / where (interchangeable)\n",
    "print(\"Engineering employees earning > 95k:\")\n",
    "employees.filter(\n",
    "    (col(\"department\") == \"Engineering\") & (col(\"salary\") > 95000)\n",
    ").show()\n",
    "\n",
    "# isin\n",
    "print(\"Marketing or HR employees:\")\n",
    "employees.where(col(\"department\").isin(\"Marketing\", \"HR\")).show()\n",
    "\n",
    "# String pattern match\n",
    "print(\"Names containing 'a' (case-insensitive):\")\n",
    "employees.filter(col(\"name\").rlike(\"(?i)a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Aggregations & GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "section3_agg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+----------+----------+-------------+\n",
      "| department|headcount|avg_salary|min_salary|max_salary|stddev_salary|\n",
      "+-----------+---------+----------+----------+----------+-------------+\n",
      "|Engineering|        4|  102000.0|   95000.0|  110000.0|       6782.0|\n",
      "|         HR|        2|   61500.0|   60000.0|   63000.0|       2121.0|\n",
      "|  Marketing|        2|   70000.0|   68000.0|   72000.0|       2828.0|\n",
      "+-----------+---------+----------+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, min, max, stddev, round as spark_round\n",
    "\n",
    "dept_stats = (\n",
    "    employees\n",
    "    .groupBy(\"department\")\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"headcount\"),\n",
    "        spark_round(avg(\"salary\"), 0).alias(\"avg_salary\"),\n",
    "        min(\"salary\").alias(\"min_salary\"),\n",
    "        max(\"salary\").alias(\"max_salary\"),\n",
    "        spark_round(stddev(\"salary\"), 0).alias(\"stddev_salary\"),\n",
    "    )\n",
    "    .orderBy(\"department\")\n",
    ")\n",
    "\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "section3_pivot",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+\n",
      "|Engineering|     HR|Marketing|\n",
      "+-----------+-------+---------+\n",
      "|   102000.0|61500.0|  70000.0|\n",
      "+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pivot: department salaries as columns (requires small cardinality)\n",
    "from pyspark.sql.functions import avg as avg_fn\n",
    "\n",
    "employees.groupBy().pivot(\"department\").agg(spark_round(avg_fn(\"salary\"), 0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "section4_joins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+------+-------------+\n",
      "| name| department|  salary|budget|     location|\n",
      "+-----+-----------+--------+------+-------------+\n",
      "| Hank|Engineering| 98000.0|500000|San Francisco|\n",
      "|  Eve|Engineering|105000.0|500000|San Francisco|\n",
      "|Carol|Engineering|110000.0|500000|San Francisco|\n",
      "|Alice|Engineering| 95000.0|500000|San Francisco|\n",
      "| Dave|  Marketing| 68000.0|200000|     New York|\n",
      "|  Bob|  Marketing| 72000.0|200000|     New York|\n",
      "|Grace|         HR| 63000.0|150000|      Chicago|\n",
      "|Frank|         HR| 60000.0|150000|      Chicago|\n",
      "+-----+-----------+--------+------+-------------+\n",
      "\n",
      "+-----+-----------+-------------+\n",
      "| name| department|     location|\n",
      "+-----+-----------+-------------+\n",
      "|Alice|Engineering|San Francisco|\n",
      "|  Bob|  Marketing|     New York|\n",
      "|Carol|Engineering|San Francisco|\n",
      "| Dave|  Marketing|     New York|\n",
      "|  Eve|Engineering|San Francisco|\n",
      "|Frank|         HR|      Chicago|\n",
      "|Grace|         HR|      Chicago|\n",
      "| Hank|Engineering|San Francisco|\n",
      "+-----+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inner join â€“ enrich employees with department metadata\n",
    "enriched = employees.join(departments, employees.department == departments.dept_id, how=\"inner\")\n",
    "enriched.select(\"name\", \"department\", \"salary\", \"budget\", \"location\").show()\n",
    "\n",
    "# Left join (employees without a matching department row still appear)\n",
    "employees_left = employees.join(departments, employees.department == departments.dept_id, how=\"left\")\n",
    "employees_left.select(\"name\", \"department\", \"location\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "section4_self_join",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+\n",
      "|employee_1|employee_2| department|\n",
      "+----------+----------+-----------+\n",
      "|     Alice|      Hank|Engineering|\n",
      "|     Alice|       Eve|Engineering|\n",
      "|     Alice|     Carol|Engineering|\n",
      "|     Carol|      Hank|Engineering|\n",
      "|     Carol|       Eve|Engineering|\n",
      "|       Eve|      Hank|Engineering|\n",
      "|     Frank|     Grace|         HR|\n",
      "|       Bob|      Dave|  Marketing|\n",
      "+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Self-join: pair employees in the same department\n",
    "emp_a = employees.alias(\"a\")\n",
    "emp_b = employees.alias(\"b\")\n",
    "\n",
    "(\n",
    "    emp_a.join(emp_b,\n",
    "               (col(\"a.department\") == col(\"b.department\")) & (col(\"a.id\") < col(\"b.id\")),\n",
    "               how=\"inner\")\n",
    "    .select(\n",
    "        col(\"a.name\").alias(\"employee_1\"),\n",
    "        col(\"b.name\").alias(\"employee_2\"),\n",
    "        col(\"a.department\")\n",
    "    )\n",
    "    .orderBy(\"department\", \"employee_1\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Window Functions\n",
    "\n",
    "Window functions compute a value for each row based on a group of related rows (the *window*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "section5_rank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+----+----------+-------+\n",
      "| name| department|  salary|rank|dense_rank|row_num|\n",
      "+-----+-----------+--------+----+----------+-------+\n",
      "|Carol|Engineering|110000.0|   1|         1|      1|\n",
      "|  Eve|Engineering|105000.0|   2|         2|      2|\n",
      "| Hank|Engineering| 98000.0|   3|         3|      3|\n",
      "|Alice|Engineering| 95000.0|   4|         4|      4|\n",
      "|Grace|         HR| 63000.0|   1|         1|      1|\n",
      "|Frank|         HR| 60000.0|   2|         2|      2|\n",
      "|  Bob|  Marketing| 72000.0|   1|         1|      1|\n",
      "| Dave|  Marketing| 68000.0|   2|         2|      2|\n",
      "+-----+-----------+--------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, dense_rank, row_number, lag, lead, sum as sum_fn, avg as avg_fn2\n",
    "\n",
    "# Partition by department, order by salary descending\n",
    "dept_window = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    rank().over(dept_window).alias(\"rank\"),\n",
    "    dense_rank().over(dept_window).alias(\"dense_rank\"),\n",
    "    row_number().over(dept_window).alias(\"row_num\"),\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "section5_lag_lead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+-----------+-----------+\n",
      "| name| department|  salary|prev_salary|next_salary|\n",
      "+-----+-----------+--------+-----------+-----------+\n",
      "|Carol|Engineering|110000.0|       NULL|   105000.0|\n",
      "|  Eve|Engineering|105000.0|   110000.0|    98000.0|\n",
      "| Hank|Engineering| 98000.0|   105000.0|    95000.0|\n",
      "|Alice|Engineering| 95000.0|    98000.0|       NULL|\n",
      "|Grace|         HR| 63000.0|       NULL|    60000.0|\n",
      "|Frank|         HR| 60000.0|    63000.0|       NULL|\n",
      "|  Bob|  Marketing| 72000.0|       NULL|    68000.0|\n",
      "| Dave|  Marketing| 68000.0|    72000.0|       NULL|\n",
      "+-----+-----------+--------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lag / lead: compare salary to next/previous colleague in same dept\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"salary\",\n",
    "    lag(\"salary\", 1).over(dept_window).alias(\"prev_salary\"),\n",
    "    lead(\"salary\", 1).over(dept_window).alias(\"next_salary\"),\n",
    ").orderBy(\"department\", col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "section5_running_total",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+----------+--------+-----------------+\n",
      "| name| department| hire_date|  salary|cumulative_salary|\n",
      "+-----+-----------+----------+--------+-----------------+\n",
      "|Carol|Engineering|2017-11-20|110000.0|         110000.0|\n",
      "|  Eve|Engineering|2018-05-05|105000.0|         215000.0|\n",
      "|Alice|Engineering|2019-03-15| 95000.0|         310000.0|\n",
      "| Hank|Engineering|2019-06-01| 98000.0|         408000.0|\n",
      "|Grace|         HR|2020-09-14| 63000.0|          63000.0|\n",
      "|Frank|         HR|2022-02-28| 60000.0|         123000.0|\n",
      "|  Bob|  Marketing|2020-07-01| 72000.0|          72000.0|\n",
      "| Dave|  Marketing|2021-01-10| 68000.0|         140000.0|\n",
      "+-----+-----------+----------+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Running total of salary within department (order matters)\n",
    "running_window = Window.partitionBy(\"department\").orderBy(\"hire_date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"department\", \"hire_date\", \"salary\",\n",
    "    sum_fn(\"salary\").over(running_window).alias(\"cumulative_salary\")\n",
    ").orderBy(\"department\", \"hire_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Spark SQL\n",
    "\n",
    "Register DataFrames as temp views and query with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "section6_sql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------+\n",
      "| department| name|  salary|\n",
      "+-----------+-----+--------+\n",
      "|Engineering|Carol|110000.0|\n",
      "|         HR|Grace| 63000.0|\n",
      "|  Marketing|  Bob| 72000.0|\n",
      "+-----------+-----+--------+\n",
      "\n",
      "+-----------+------+-------------+---------+-------------+----------------+\n",
      "|    dept_id|budget|     location|headcount|total_payroll|remaining_budget|\n",
      "+-----------+------+-------------+---------+-------------+----------------+\n",
      "|Engineering|500000|San Francisco|        4|     408000.0|         92000.0|\n",
      "|         HR|150000|      Chicago|        2|     123000.0|         27000.0|\n",
      "|  Marketing|200000|     New York|        2|     140000.0|         60000.0|\n",
      "+-----------+------+-------------+---------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees.createOrReplaceTempView(\"employees\")\n",
    "departments.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Top earner per department\n",
    "spark.sql(\"\"\"\n",
    "    SELECT department, name, salary\n",
    "    FROM (\n",
    "        SELECT department, name, salary,\n",
    "               RANK() OVER (PARTITION BY department ORDER BY salary DESC) AS rnk\n",
    "        FROM employees\n",
    "    )\n",
    "    WHERE rnk = 1\n",
    "    ORDER BY department\n",
    "\"\"\").show()\n",
    "\n",
    "# Department budget vs total payroll\n",
    "spark.sql(\"\"\"\n",
    "    SELECT d.dept_id, d.budget, d.location,\n",
    "           COUNT(e.id)              AS headcount,\n",
    "           ROUND(SUM(e.salary), 0)  AS total_payroll,\n",
    "           ROUND(d.budget - SUM(e.salary), 0) AS remaining_budget\n",
    "    FROM departments d\n",
    "    LEFT JOIN employees e ON e.department = d.dept_id\n",
    "    GROUP BY d.dept_id, d.budget, d.location\n",
    "    ORDER BY d.dept_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. User-Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "section7_udf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+------+\n",
      "| name|  salary|  band|\n",
      "+-----+--------+------+\n",
      "|Alice| 95000.0|band-C|\n",
      "|  Bob| 72000.0|band-B|\n",
      "|Carol|110000.0|band-C|\n",
      "| Dave| 68000.0|band-A|\n",
      "|  Eve|105000.0|band-C|\n",
      "|Frank| 60000.0|band-A|\n",
      "|Grace| 63000.0|band-A|\n",
      "| Hank| 98000.0|band-C|\n",
      "+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# --- Row-level Python UDF (slower, but flexible) ---\n",
    "def salary_band(salary: float) -> str:\n",
    "    if salary is None:\n",
    "        return \"unknown\"\n",
    "    if salary < 70000:\n",
    "        return \"band-A\"\n",
    "    if salary < 90000:\n",
    "        return \"band-B\"\n",
    "    return \"band-C\"\n",
    "\n",
    "salary_band_udf = udf(salary_band, StringType())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"salary\",\n",
    "    salary_band_udf(col(\"salary\")).alias(\"band\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "section7_pandas_udf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Vectorised Pandas UDF (much faster for large data) ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pandas_udf\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;129m@pandas_udf\u001b[39m(DoubleType())\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalise_salary\u001b[39m(series: pd.Series) -> pd.Series:\n\u001b[32m      7\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Normalise salaries to 0-1 range within each batch.\"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# --- Vectorised Pandas UDF (much faster for large data) ---\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def normalise_salary(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalise salaries to 0-1 range within each batch.\"\"\"\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "employees.select(\n",
    "    \"name\", \"salary\",\n",
    "    spark_round(normalise_salary(col(\"salary\")), 4).alias(\"norm_salary\")\n",
    ").orderBy(\"salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Null Handling & Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section8_nulls",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, isnull, isnan, count as count_fn\n",
    "\n",
    "# Introduce some nulls for demonstration\n",
    "dirty_data = [\n",
    "    (1, \"Alice\",  95000.0),\n",
    "    (2, \"Bob\",    None),\n",
    "    (3, None,     72000.0),\n",
    "    (4, \"Dave\",   float(\"nan\")),\n",
    "    (5, \"Eve\",    105000.0),\n",
    "]\n",
    "dirty_df = spark.createDataFrame(dirty_data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "print(\"=== Raw data ===\")\n",
    "dirty_df.show()\n",
    "\n",
    "# Count nulls per column\n",
    "print(\"=== Null counts ===\")\n",
    "dirty_df.select([\n",
    "    count_fn(when(isnull(c) | isnan(c), c)).alias(c)\n",
    "    for c in dirty_df.columns\n",
    "]).show()\n",
    "\n",
    "# Fill / drop\n",
    "print(\"=== After fillna ===\")\n",
    "dirty_df.fillna({\"name\": \"Unknown\", \"salary\": 0.0}).show()\n",
    "\n",
    "print(\"=== After dropna ===\")\n",
    "dirty_df.dropna().show()\n",
    "\n",
    "# coalesce: first non-null wins\n",
    "print(\"=== coalesce salary with default ===\")\n",
    "dirty_df.select(\n",
    "    \"name\",\n",
    "    coalesce(col(\"salary\"), lit(50000.0)).alias(\"salary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. String & Date Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section9_strings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    upper, lower, trim, length, concat, concat_ws,\n",
    "    regexp_replace, split, substring, initcap\n",
    ")\n",
    "\n",
    "employees.select(\n",
    "    upper(col(\"name\")).alias(\"name_upper\"),\n",
    "    length(col(\"name\")).alias(\"name_len\"),\n",
    "    concat_ws(\" @ \", col(\"name\"), col(\"department\")).alias(\"name_dept\"),\n",
    "    regexp_replace(col(\"department\"), \"Engineering\", \"Eng\").alias(\"dept_short\"),\n",
    "    substring(col(\"name\"), 1, 3).alias(\"initials\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section9_dates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    year, month, dayofweek, datediff, months_between,\n",
    "    current_date, date_add, date_format, to_date\n",
    ")\n",
    "\n",
    "employees.select(\n",
    "    \"name\",\n",
    "    \"hire_date\",\n",
    "    year(\"hire_date\").alias(\"hire_year\"),\n",
    "    month(\"hire_date\").alias(\"hire_month\"),\n",
    "    dayofweek(\"hire_date\").alias(\"day_of_week\"),          # 1=Sunday\n",
    "    datediff(current_date(), col(\"hire_date\")).alias(\"days_employed\"),\n",
    "    spark_round(months_between(current_date(), col(\"hire_date\")), 1).alias(\"months_employed\"),\n",
    "    date_format(col(\"hire_date\"), \"MMM dd, yyyy\").alias(\"formatted\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Reading & Writing Data\n",
    "\n",
    "Write to common formats and read them back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, os\n",
    "\n",
    "tmp = tempfile.mkdtemp()\n",
    "\n",
    "# --- Parquet (columnar, compressed, schema-preserving) ---\n",
    "parquet_path = os.path.join(tmp, \"employees.parquet\")\n",
    "employees.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "df_from_parquet = spark.read.parquet(parquet_path)\n",
    "print(\"Read back from Parquet:\")\n",
    "df_from_parquet.printSchema()\n",
    "df_from_parquet.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CSV ---\n",
    "csv_path = os.path.join(tmp, \"employees.csv\")\n",
    "employees.write.mode(\"overwrite\").option(\"header\", True).csv(csv_path)\n",
    "\n",
    "df_from_csv = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(csv_path)\n",
    "print(\"Read back from CSV:\")\n",
    "df_from_csv.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "section10_partitioned",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Partitioned write (Hive-style) ---\n",
    "partitioned_path = os.path.join(tmp, \"employees_by_dept\")\n",
    "(\n",
    "    employees\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"department\")\n",
    "    .parquet(partitioned_path)\n",
    ")\n",
    "\n",
    "# Spark automatically prunes partitions when filtering on the partition column\n",
    "df_eng = spark.read.parquet(partitioned_path).filter(col(\"department\") == \"Engineering\")\n",
    "print(\"Engineering employees (partition-pruned read):\")\n",
    "df_eng.show()\n",
    "\n",
    "print(f\"\\nTemp files written to: {tmp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "teardown_header",
   "metadata": {},
   "source": [
    "---\n",
    "## Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teardown_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session when done to release resources\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (exfiles-spark)",
   "language": "python",
   "name": "exfiles-spark311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
